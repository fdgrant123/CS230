{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"TeamDataset_7_50.csv\").drop(columns = [\"Unnamed: 0\"])\n",
    "\n",
    "dataset[\"Y\"] = dataset[\"Y\"].apply(lambda x: x*-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Partition Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightSeason(train):\n",
    "    additional_x = pd.DataFrame(columns = train.columns)\n",
    "    for i in range(train.shape[0]):\n",
    "        game = train.iloc[[i]]\n",
    "        year = int(str(train[\"Date\"].iloc[i])[:4])\n",
    "        multiplier = year - 2013\n",
    "        for i in range(multiplier):\n",
    "            additional_x = pd.concat([additional_x, game], axis = 0)\n",
    "\n",
    "    train = pd.concat([train, additional_x], axis = 0)\n",
    "    train = train.sort_values(by=['Date'])\n",
    "    train = train.reset_index(drop=True)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weightTeam(train, team):\n",
    "    multiplier = 5\n",
    "    additional_x = pd.DataFrame(columns = train.columns)\n",
    "    for i in range(train.shape[0]):\n",
    "        game = train.iloc[[i]]\n",
    "        if game[\"Home\"].iloc[0] == team or game[\"Away\"].iloc[0] == team:\n",
    "            for i in range(multiplier):\n",
    "                additional_x = pd.concat([additional_x, game], axis = 0)\n",
    "\n",
    "    train = pd.concat([train, additional_x], axis = 0)\n",
    "    train = train.sort_values(by=['Date'])\n",
    "    train = train.reset_index(drop=True)\n",
    "    return train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done with ORL\n",
      "Done with IND\n",
      "Done with MIA\n",
      "Done with CHI\n",
      "Done with LAL\n",
      "Done with LAC\n",
      "Done with CLE\n",
      "Done with BKN\n",
      "Done with PHI\n",
      "Done with TOR\n",
      "Done with BOS\n",
      "Done with WAS\n",
      "Done with DET\n",
      "Done with NYK\n",
      "Done with MIL\n",
      "Done with HOU\n",
      "Done with CHA\n",
      "Done with MIN\n",
      "Done with NOP\n",
      "Done with DAL\n",
      "Done with ATL\n",
      "Done with SAS\n",
      "Done with MEM\n",
      "Done with UTA\n",
      "Done with OKC\n",
      "Done with POR\n",
      "Done with PHX\n",
      "Done with SAC\n",
      "Done with DEN\n",
      "Done with GSW\n"
     ]
    }
   ],
   "source": [
    "teams = [\"ORL\", \"IND\", \"MIA\", \"CHI\", \"LAL\", \"LAC\", \n",
    "         \"CLE\", \"BKN\", \"PHI\", \"TOR\", \"BOS\", \"WAS\", \n",
    "         \"DET\", \"NYK\", \"MIL\", \"HOU\", \"CHA\", \"MIN\", \n",
    "         \"NOP\", \"DAL\", \"ATL\", \"SAS\", \"MEM\", \"UTA\", \n",
    "         \"OKC\", \"POR\", \"PHX\", \"SAC\", \"DEN\", \"GSW\"]\n",
    "\n",
    "datasets_train = []\n",
    "\n",
    "\n",
    "train = dataset[dataset[\"Date\"] < 20180800]  \n",
    "train = weightSeason(train)\n",
    "for team in teams:\n",
    "    team_data = weightTeam(train, team)\n",
    "    team_x = team_data.iloc[:, 5:]\n",
    "    team_y = team_data.iloc[:, 4:5]\n",
    "    team_identifiers = team_data.iloc[:, :4]\n",
    "    datasets_train.append((team_x.to_numpy(), team_y.to_numpy().reshape(team_y.shape[0], 1), team_identifiers))\n",
    "    print(\"Done with {}\".format(team))\n",
    "    \n",
    "#Validation data includes 2018-2019 season\n",
    "val = dataset[dataset[\"Date\"] > 20180800] \n",
    "val = val[val[\"Date\"] < 20190800] \n",
    "val_x = val.iloc[:, 5:]\n",
    "val_y = val.iloc[:, 4:5]\n",
    "val_identifiers = val.iloc[:, :4]\n",
    "dataset_val = (val_x.to_numpy(), val_y.to_numpy().reshape(val.shape[0], 1), val_identifiers)\n",
    "\n",
    "#Test data includes 2019-2020\n",
    "test = dataset[dataset[\"Date\"] > 20190800]  \n",
    "test_x = test.iloc[:, 5:]\n",
    "test_y = test.iloc[:, 4:5]\n",
    "test_identifiers = test.iloc[:, :4]\n",
    "dataset_test = (test_x.to_numpy(), test_y.to_numpy().reshape(test.shape[0], 1), test_identifiers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207.4231565123272\n",
      "202.17833445659744\n"
     ]
    }
   ],
   "source": [
    "#Simple Model --> Predict mean for every example\n",
    "\n",
    "\n",
    "prediction_value = np.mean(dataset_train[1])\n",
    "val_mse = np.mean((np.ones((dataset_val[1].shape[0], 1)) * prediction_value - dataset_val[1])**2)\n",
    "test_mse = np.mean((np.ones((dataset_test[1].shape[0], 1)) * prediction_value - dataset_test[1])**2)\n",
    "\n",
    "print(val_mse)\n",
    "print(test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Model (Linear Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141.60834667966134\n",
      "177.34653283255673\n",
      "162.43834237062939\n"
     ]
    }
   ],
   "source": [
    "weights = np.matmul(np.linalg.pinv(dataset_train[0]), dataset_train[1])\n",
    "predictions = np.matmul(dataset_test[0], weights)\n",
    "test_mse = np.mean((predictions - dataset_test[1])**2)\n",
    "\n",
    "train_pred = np.matmul(dataset_train[0], weights)\n",
    "train_mse = np.mean((train_pred - dataset_train[1])**2)\n",
    "\n",
    "val_pred = np.matmul(dataset_val[0], weights)\n",
    "val_mse = np.mean((val_pred - dataset_val[1])**2)\n",
    "\n",
    "print(train_mse)\n",
    "print(val_mse)\n",
    "print(test_mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sportbook MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for open is 150.8273656606694\n",
      "MSE for close is 148.14847305389222\n"
     ]
    }
   ],
   "source": [
    "lines = pd.read_csv(\"OfficialLines.csv\")\n",
    "games = pd.read_csv(\"Master_GameList.csv\")\n",
    "\n",
    "mse_open = []\n",
    "mse_close = []\n",
    "\n",
    "for i in range(lines.shape[0]):\n",
    "    game_id = lines[\"Game\"].iloc[i]\n",
    "    \n",
    "    spread_open = float(lines[\"Open_Spread_Home\"].iloc[i])\n",
    "    spread_close = float(lines[\"Close_Spread_Home\"].iloc[i])\n",
    "                         \n",
    "    team_game = games[games[\"GAME_ID\"] == game_id]\n",
    "    if team_game[\"TEAM_ABBREVIATION\"].iloc[0] == team_game[\"Home\"].iloc[0]:\n",
    "        home_real = team_game[\"PTS\"].iloc[0]\n",
    "        away_real = team_game[\"PTS\"].iloc[1]\n",
    "                         \n",
    "    else:\n",
    "        home_real = team_game[\"PTS\"].iloc[1]\n",
    "        away_real = team_game[\"PTS\"].iloc[0]\n",
    "        \n",
    "    spread_real = away_real - home_real\n",
    "    \n",
    "    open_val = (spread_real - spread_open)**2\n",
    "    mse_open.append(open_val)\n",
    "        \n",
    "    close_val = (spread_real - spread_close)**2\n",
    "    \n",
    "    if close_val > -1 and close_val < 10000:\n",
    "        mse_close.append(close_val)\n",
    "    \n",
    "print(\"MSE for open is {}\".format(np.mean(mse_open)))\n",
    "print(\"MSE for close is {}\".format(np.mean(mse_close)))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "small = [(19, \"DAL\"), (23, \"UTA\")]\n",
    "\n",
    "#for i, team in enumerate(teams):\n",
    "for i, team in small:\n",
    "    checkpoint_filepath = 'checkpoint_bigVal/{}'.format(team)\n",
    "    checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath = checkpoint_filepath, monitor='val_loss', verbose=0, \n",
    "                                                    save_best_only=True, save_weights_only=True, mode='min', \n",
    "                                                    save_freq='epoch')\n",
    "    train_x = datasets_train[i][0]\n",
    "    train_x = np.concatenate((train_x, dataset_val[0]), axis = 0)\n",
    "    \n",
    "    train_y = datasets_train[i][1]\n",
    "    train_y = np.concatenate((train_y, dataset_val[1]), axis = 0)\n",
    "\n",
    "    model = tf.keras.Sequential()\n",
    "    NAME = \"Model-{}-{}-{}\".format(\"Weighted\", team, \"Expanded\")\n",
    "    tensorboard = TensorBoard(log_dir = 'Models/{}'.format(NAME))\n",
    "    model.add(tf.keras.layers.Input(shape=(train_x.shape[1],)))\n",
    "    model.add(tf.keras.layers.Dense(60, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(30, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(1))\n",
    "    optimizer = tf.keras.optimizers.Adam(0.00001)\n",
    "    model.compile(loss='mse', optimizer=optimizer)\n",
    "    \n",
    "    split = (dataset_val[0].shape[0] / train_x.shape[0])\n",
    "    model.fit(train_x, train_y, epochs = 400, verbose=0, batch_size = 64, callbacks = [tensorboard, checkpoint], \n",
    "              validation_split = split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ORL\n",
      "1254/1254 - 1s - loss: 151.6248\n",
      "39/39 - 0s - loss: 175.3871\n",
      "31/31 - 0s - loss: 176.6796\n",
      "\n",
      "IND\n",
      "1254/1254 - 2s - loss: 150.4563\n",
      "39/39 - 0s - loss: 174.9926\n",
      "31/31 - 0s - loss: 182.7012\n",
      "\n",
      "MIA\n",
      "1254/1254 - 1s - loss: 151.4589\n",
      "39/39 - 0s - loss: 174.5072\n",
      "31/31 - 0s - loss: 177.5104\n",
      "\n",
      "CHI\n",
      "1254/1254 - 1s - loss: 151.3304\n",
      "39/39 - 0s - loss: 173.3978\n",
      "31/31 - 0s - loss: 175.6337\n",
      "\n",
      "LAL\n",
      "1254/1254 - 1s - loss: 151.9726\n",
      "39/39 - 0s - loss: 175.6748\n",
      "31/31 - 0s - loss: 174.7802\n",
      "\n",
      "LAC\n",
      "1254/1254 - 2s - loss: 153.2101\n",
      "39/39 - 0s - loss: 177.8969\n",
      "31/31 - 0s - loss: 179.3354\n",
      "\n",
      "CLE\n",
      "1254/1254 - 1s - loss: 153.0576\n",
      "39/39 - 0s - loss: 177.7879\n",
      "31/31 - 0s - loss: 188.5524\n",
      "\n",
      "BKN\n",
      "1254/1254 - 1s - loss: 151.2906\n",
      "39/39 - 0s - loss: 175.6936\n",
      "31/31 - 0s - loss: 176.8442\n",
      "\n",
      "PHI\n",
      "1254/1254 - 1s - loss: 151.8486\n",
      "39/39 - 0s - loss: 173.0894\n",
      "31/31 - 0s - loss: 175.0081\n",
      "\n",
      "TOR\n",
      "1254/1254 - 1s - loss: 149.4034\n",
      "39/39 - 0s - loss: 174.7312\n",
      "31/31 - 0s - loss: 175.7889\n",
      "\n",
      "BOS\n",
      "1254/1254 - 1s - loss: 149.8415\n",
      "39/39 - 0s - loss: 173.1047\n",
      "31/31 - 0s - loss: 178.1942\n",
      "\n",
      "WAS\n",
      "1254/1254 - 1s - loss: 150.7912\n",
      "39/39 - 0s - loss: 173.1266\n",
      "31/31 - 0s - loss: 175.3593\n",
      "\n",
      "DET\n",
      "1254/1254 - 1s - loss: 152.5545\n",
      "39/39 - 0s - loss: 177.5273\n",
      "31/31 - 0s - loss: 178.8932\n",
      "\n",
      "NYK\n",
      "1254/1254 - 1s - loss: 155.8948\n",
      "39/39 - 0s - loss: 178.6203\n",
      "31/31 - 0s - loss: 179.5079\n",
      "\n",
      "MIL\n",
      "1254/1254 - 1s - loss: 148.9210\n",
      "39/39 - 0s - loss: 172.2190\n",
      "31/31 - 0s - loss: 173.6369\n",
      "\n",
      "HOU\n",
      "1254/1254 - 1s - loss: 151.4236\n",
      "39/39 - 0s - loss: 175.1353\n",
      "31/31 - 0s - loss: 181.5621\n",
      "\n",
      "CHA\n",
      "1254/1254 - 1s - loss: 153.0336\n",
      "39/39 - 0s - loss: 175.1123\n",
      "31/31 - 0s - loss: 176.5133\n",
      "\n",
      "MIN\n",
      "1254/1254 - 1s - loss: 150.2783\n",
      "39/39 - 0s - loss: 174.5146\n",
      "31/31 - 0s - loss: 184.8428\n",
      "\n",
      "NOP\n",
      "1254/1254 - 1s - loss: 153.7430\n",
      "39/39 - 0s - loss: 176.7959\n",
      "31/31 - 0s - loss: 177.9932\n",
      "\n",
      "DAL\n",
      "1254/1254 - 2s - loss: 150.2763\n",
      "39/39 - 0s - loss: 174.0366\n",
      "31/31 - 0s - loss: 177.2685\n",
      "\n",
      "ATL\n",
      "1254/1254 - 2s - loss: 150.8860\n",
      "39/39 - 0s - loss: 175.7829\n",
      "31/31 - 0s - loss: 178.7127\n",
      "\n",
      "SAS\n",
      "1254/1254 - 2s - loss: 151.7742\n",
      "39/39 - 0s - loss: 173.4971\n",
      "31/31 - 0s - loss: 175.5703\n",
      "\n",
      "MEM\n",
      "1254/1254 - 2s - loss: 151.5526\n",
      "39/39 - 0s - loss: 174.5279\n",
      "31/31 - 0s - loss: 175.2417\n",
      "\n",
      "UTA\n",
      "1254/1254 - 1s - loss: 151.6452\n",
      "39/39 - 0s - loss: 174.2855\n",
      "31/31 - 0s - loss: 176.8572\n",
      "\n",
      "OKC\n",
      "1254/1254 - 1s - loss: 150.3935\n",
      "39/39 - 0s - loss: 174.0385\n",
      "31/31 - 0s - loss: 177.2396\n",
      "\n",
      "POR\n",
      "1254/1254 - 1s - loss: 150.4673\n",
      "39/39 - 0s - loss: 176.4143\n",
      "31/31 - 0s - loss: 180.6421\n",
      "\n",
      "PHX\n",
      "1254/1254 - 2s - loss: 152.3821\n",
      "39/39 - 0s - loss: 175.7652\n",
      "31/31 - 0s - loss: 178.1652\n",
      "\n",
      "SAC\n",
      "1254/1254 - 2s - loss: 151.8046\n",
      "39/39 - 0s - loss: 176.3520\n",
      "31/31 - 0s - loss: 178.0919\n",
      "\n",
      "DEN\n",
      "1254/1254 - 2s - loss: 151.9451\n",
      "39/39 - 0s - loss: 175.6147\n",
      "31/31 - 0s - loss: 182.0161\n",
      "\n",
      "GSW\n",
      "1254/1254 - 1s - loss: 153.5014\n",
      "39/39 - 0s - loss: 175.6331\n",
      "31/31 - 0s - loss: 175.6513\n"
     ]
    }
   ],
   "source": [
    "for team in teams:\n",
    "    checkpoint_filepath = 'checkpoint_bigVal/{}'.format(team)\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    print(\"\\n{}\".format(team))\n",
    "    loss_train = model.evaluate(dataset_train[0], dataset_train[1], verbose = 2)\n",
    "    loss_val = model.evaluate(dataset_val[0], dataset_val[1], verbose = 2)\n",
    "    loss_test = model.evaluate(datasets_test[0],  datasets_test[1], verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Model Agreement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getAccuracy(predict_home, predict_away, line, outcome, conf):\n",
    "    if predict_home > line and predict_away < line:\n",
    "        return None\n",
    "    elif predict_home < line and predict_away > line:\n",
    "        return None\n",
    "    elif (predict_home - line) <= conf or (predict_away - line) <= conf:\n",
    "        return None\n",
    "    elif predict_home > line and predict_away > line:\n",
    "        if outcome > line:\n",
    "            return 1\n",
    "        else: return 0\n",
    "    else:\n",
    "        if outcome < line:\n",
    "            return 1\n",
    "        else: return 0 \n",
    "        \n",
    "def printStats(stats):\n",
    "    print(\"The model achieved predictive accuracy of {} on opening spreads\".format(round(stats[0], 3)))\n",
    "    print(\"The model achieved predictive accuracy of {} on closing spreads\".format(round(stats[1], 3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = pd.read_csv(\"OfficialLines.csv\")\n",
    "\n",
    "counter_open = 0\n",
    "counter_close = 0\n",
    "open_spreads = []\n",
    "close_spreads = []\n",
    "conf = 4\n",
    "\n",
    "for i in range(datasets_test[2].shape[0]):\n",
    "    game = datasets_test[2][\"Game\"].iloc[i]\n",
    "    home = datasets_test[2][\"Home\"].iloc[i]\n",
    "    away = datasets_test[2][\"Away\"].iloc[i]\n",
    "    \n",
    "    #Actual Outcome\n",
    "    spread_real = datasets_test[1][i, :][0]\n",
    "    \n",
    "    \n",
    "    game_features = datasets_test[0][i, :].reshape(1, datasets_test[0].shape[1])\n",
    "    \n",
    "    #Home Model\n",
    "    checkpoint_filepath = 'checkpoint/{}'.format(home)\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    predict_home = model.predict(game_features)[0, 0]\n",
    "\n",
    "    #Away Model\n",
    "    checkpoint_filepath = 'checkpoint/{}'.format(away)\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    predict_away = model.predict(game_features)[0, 0]\n",
    "\n",
    "    #Sportsbook Prediction\n",
    "    line = lines[lines[\"Game\"] == game]\n",
    "    spread_open = float(line[\"Open_Spread_Home\"].iloc[0])\n",
    "    spread_close = float(line[\"Close_Spread_Home\"].iloc[0])\n",
    "    \n",
    "    #Assess open lines\n",
    "    acc_open = getAccuracy(predict_home, predict_away, spread_open, spread_real, conf)\n",
    "    if acc_open != None:\n",
    "        counter_open += 1\n",
    "        open_spreads.append(acc_open)\n",
    "        \n",
    "    #Assess close lines\n",
    "    acc_close = getAccuracy(predict_home, predict_away, spread_close, spread_real, conf)\n",
    "    if acc_close != None:\n",
    "        counter_close += 1\n",
    "        close_spreads.append(acc_close)\n",
    "\n",
    "print(\"Open is {} percent of games\".format(100 * (counter_open / datasets_test[2].shape[0])))\n",
    "print(\"Close is {} percent of games\".format(100 * (counter_close / datasets_test[2].shape[0])))\n",
    "final_stats = [np.mean(open_spreads), np.mean(close_spreads)]\n",
    "printStats(final_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Compute MSE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "917/917 - 1s - loss: 150.0759\n",
      "39/39 - 0s - loss: 172.4235\n",
      "917/917 - 1s - loss: 148.6202\n",
      "39/39 - 0s - loss: 173.0741\n",
      "917/917 - 1s - loss: 145.0933\n",
      "39/39 - 0s - loss: 174.9758\n",
      "918/918 - 1s - loss: 153.0775\n",
      "39/39 - 0s - loss: 174.4477\n",
      "917/917 - 1s - loss: 147.3410\n",
      "39/39 - 0s - loss: 173.9798\n",
      "916/916 - 1s - loss: 149.2510\n",
      "39/39 - 0s - loss: 175.4232\n",
      "918/918 - 1s - loss: 154.7843\n",
      "39/39 - 0s - loss: 178.0298\n",
      "918/918 - 1s - loss: 149.9467\n",
      "39/39 - 0s - loss: 176.1398\n",
      "918/918 - 1s - loss: 144.8386\n",
      "39/39 - 0s - loss: 172.7151\n",
      "918/918 - 1s - loss: 141.4887\n",
      "39/39 - 0s - loss: 171.3109\n",
      "917/917 - 1s - loss: 140.3386\n",
      "39/39 - 0s - loss: 170.2478\n",
      "919/919 - 1s - loss: 146.8316\n",
      "39/39 - 0s - loss: 170.7331\n",
      "917/917 - 1s - loss: 157.4682\n",
      "39/39 - 0s - loss: 175.1130\n",
      "917/917 - 1s - loss: 143.3578\n",
      "39/39 - 0s - loss: 172.5170\n",
      "918/918 - 1s - loss: 140.7502\n",
      "39/39 - 0s - loss: 173.0145\n",
      "917/917 - 1s - loss: 148.0063\n",
      "39/39 - 0s - loss: 173.5927\n",
      "917/917 - 1s - loss: 152.0322\n",
      "39/39 - 0s - loss: 173.9947\n",
      "918/918 - 1s - loss: 144.7567\n",
      "39/39 - 0s - loss: 172.8202\n",
      "918/918 - 1s - loss: 146.3620\n",
      "39/39 - 0s - loss: 171.7052\n",
      "917/917 - 1s - loss: 146.3275\n",
      "39/39 - 0s - loss: 174.7608\n",
      "917/917 - 1s - loss: 152.4770\n",
      "39/39 - 0s - loss: 172.7726\n",
      "917/917 - 1s - loss: 149.7991\n",
      "39/39 - 0s - loss: 172.1811\n",
      "917/917 - 1s - loss: 145.4163\n",
      "39/39 - 0s - loss: 174.0483\n",
      "917/917 - 1s - loss: 150.0135\n",
      "39/39 - 0s - loss: 174.4965\n",
      "917/917 - 1s - loss: 148.4556\n",
      "39/39 - 0s - loss: 173.9680\n",
      "917/917 - 1s - loss: 145.8313\n",
      "39/39 - 0s - loss: 172.2114\n",
      "917/917 - 1s - loss: 149.9982\n",
      "39/39 - 0s - loss: 173.8002\n",
      "918/918 - 1s - loss: 143.6150\n",
      "39/39 - 0s - loss: 174.0107\n",
      "918/918 - 1s - loss: 149.3714\n",
      "39/39 - 0s - loss: 173.7253\n",
      "917/917 - 1s - loss: 152.7265\n",
      "39/39 - 0s - loss: 174.0731\n",
      "Train MSE is 147.9484100341797\n",
      "Validation MSE is 173.54352823893228\n"
     ]
    }
   ],
   "source": [
    "train_mse = 0\n",
    "val_mse = 0\n",
    "for i, team in enumerate(teams):\n",
    "    checkpoint_filepath = 'checkpoint/{}'.format(team)\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    train_mse += model.evaluate(datasets_train[i][0], datasets_train[i][1], verbose = 2)\n",
    "    val_mse += model.evaluate(dataset_val[0], dataset_val[1], verbose = 2)\n",
    "print(\"Train MSE is {}\".format(train_mse / 30))\n",
    "print(\"Validation MSE is {}\".format(val_mse / 30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test all MSE is 174.91278164068402\n",
      "Test subset is 212.434755129812\n",
      "Book all is 163.8480947476828\n",
      "Book subset is 177.294\n",
      "Number of subset games is 125\n"
     ]
    }
   ],
   "source": [
    "lines = pd.read_csv(\"OfficialLines.csv\")\n",
    "\n",
    "mse_test_all = []\n",
    "mse_test_subset = []\n",
    "book_all = []\n",
    "book_subset = []\n",
    "game_counter = 0\n",
    "\n",
    "for i in range(dataset_test[2].shape[0]):\n",
    "    game = dataset_test[2][\"Game\"].iloc[i]\n",
    "    home = dataset_test[2][\"Home\"].iloc[i]\n",
    "    away = dataset_test[2][\"Away\"].iloc[i]\n",
    "    \n",
    "    #Actual Outcome\n",
    "    spread_real = dataset_test[1][i, :][0]\n",
    "    \n",
    "    game_features = dataset_test[0][i, :].reshape(1, dataset_test[0].shape[1])\n",
    "    \n",
    "    #Home Model\n",
    "    checkpoint_filepath = 'checkpoint/{}'.format(home)\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    predict_home = model.predict(game_features)[0, 0]\n",
    "\n",
    "    #Away Model\n",
    "    checkpoint_filepath = 'checkpoint/{}'.format(away)\n",
    "    model.load_weights(checkpoint_filepath)\n",
    "    predict_away = model.predict(game_features)[0, 0]\n",
    "\n",
    "    #Sportsbook Prediction\n",
    "    line = lines[lines[\"Game\"] == game]\n",
    "    spread_open = float(line[\"Open_Spread_Home\"].iloc[0])\n",
    "    spread_close = float(line[\"Close_Spread_Home\"].iloc[0])\n",
    "    \n",
    "    mse_test_all.append((spread_real - np.mean([predict_away, predict_home]))**2)\n",
    "    book_all.append((spread_real - spread_open)**2)\n",
    "    \n",
    "    #Assess open lines\n",
    "    acc_open = getAccuracy(predict_home, predict_away, spread_open, spread_real, conf)\n",
    "    if acc_open != None:\n",
    "        game_counter += 1\n",
    "        book_subset.append((spread_real - spread_open)**2)\n",
    "        mse_test_subset.append((spread_real - np.mean([predict_away, predict_home]))**2)\n",
    "        \n",
    "        \n",
    "print(\"Test all MSE is {}\".format(np.mean(mse_test_all)))\n",
    "print(\"Test subset is {}\".format(np.mean(mse_test_subset)))\n",
    "print(\"Book all is {}\".format(np.mean(book_all)))\n",
    "print(\"Book subset is {}\".format(np.mean(book_subset)))\n",
    "print(\"Number of subset games is {}\".format(game_counter))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
